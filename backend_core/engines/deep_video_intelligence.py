import os
import json
import time
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv(".env.local")

# 1. TRY IMPORTING HEAVY TOOLS (Graceful Failure)
try:
    import cv2
    import mediapipe as mp
    # import whisper # Whisper is too heavy for basic local install, usually better in Docker
    VISION_LIBS_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è LOCAL AI LIBS MISSING: Running in 'Gemini Vision Only' mode. (Install mediapipe/opencv-python for motion analysis)")
    VISION_LIBS_AVAILABLE = False

class DeepVideoIntelligence:
    def __init__(self):
        self.api_key = os.getenv("GEMINI_API_KEY") or os.getenv("VITE_GEMINI_API_KEY")
        self.model_id = "gemini-2.0-flash-thinking-exp-1219" # The Thinking Model
        
        # Initialize Gemini Client
        if self.api_key:
            genai.configure(api_key=self.api_key)
            self.client = genai.GenerativeModel(self.model_id)
            print(f"üß† GEMINI 2.0: Connected ({self.model_id})")
        else:
            print("‚ùå MISSING GEMINI API KEY")
            self.client = None

        # Initialize MediaPipe (If available)
        self.pose_detector = None
        if VISION_LIBS_AVAILABLE:
            try:
                self.mp_pose = mp.solutions.pose
                self.pose_detector = self.mp_pose.Pose(static_image_mode=False, model_complexity=1)
                print("‚úÖ DEEP VIDEO: Motion Analysis Engine Loaded")
            except Exception as e:
                print(f"‚ö†Ô∏è DEEP VIDEO: Motion Engine Disabled ({e})")

    def analyze_video(self, video_path: str):
        """
        Full Analysis: Visual Energy + Semantic Understanding
        """
        print(f"üëÅÔ∏è DEEP SCAN: Analyzing {video_path}...")
        
        # 1. Motion Energy (Local CPU)
        motion_score = self._analyze_motion_energy(video_path)
        
        # 2. Deep Semantic Reasoning (Gemini 2.0 Cloud)
        semantic_analysis = self._gemini_deep_think(video_path)
        
        return {
            "visual_energy_score": motion_score,
            "semantic_analysis": semantic_analysis
        }

    def _analyze_motion_energy(self, video_path):
        """Calculates how much 'movement' is in the video (0-100)"""
        if not VISION_LIBS_AVAILABLE or not os.path.exists(video_path):
            return 50 # Default baseline
            
        try:
            cap = cv2.VideoCapture(video_path)
            prev_frame = None
            total_movement = 0
            frame_count = 0
            
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret or frame_count > 150: break # Analyze first 5 seconds (~150 frames)
                
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                if prev_frame is not None:
                    diff = cv2.absdiff(prev_frame, gray)
                    non_zero = cv2.countNonZero(diff)
                    total_movement += non_zero
                
                prev_frame = gray
                frame_count += 1
                
            cap.release()
            
            # Normalize score roughly
            normalized_score = min(100, int((total_movement / (frame_count * 10000)) * 100))
            return normalized_score
        except Exception as e:
            print(f"‚ö†Ô∏è Motion Analysis Failed: {e}")
            return 50

    def _gemini_deep_think(self, video_path):
        """Uses Gemini 2.0 Flash Thinking to find the 'Why'"""
        if not self.client: return "Error: No Brain"
        
        # For this example, we assume video_path is a local file we upload
        # OR a URI. If it's a local file, we need to upload it to File API first.
        # Simplified for this snippet to assume text context or pre-uploaded URI.
        
        prompt = """
        Analyze the visual hook of this video content.
        1. What is the immediate emotion?
        2. Is there a 'Pattern Interrupt'?
        3. Rate the 'Scroll Stopping Power' (0-100).
        """
        
        # In a real scenario, we upload the file here. 
        # For now, we simulate the 'Thinking' response structure.
        return {
            "hook_type": "Detected Visual Shock",
            "thinking_trace": "The subject moves rapidly towards the camera...",
            "score": 88
        }

# Singleton
deep_video_engine = DeepVideoIntelligence()
